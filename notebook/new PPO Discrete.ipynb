{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aa0d66c-18b8-4888-bf2f-9b6ba4e0fd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import jax\n",
    "import tax\n",
    "import tree\n",
    "import optax\n",
    "import numpy as np\n",
    "import haiku as hk\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from ppo.ppo import Data\n",
    "from ppo.ppo import State\n",
    "from ppo.ppo import Batch\n",
    "from ppo.ppo import process_data\n",
    "from ppo.ppo import loss_ppo_def\n",
    "from ppo.common.utils import evaluation\n",
    "from jax import jit\n",
    "from jax import vmap\n",
    "from functools import partial\n",
    "from gym.vector import AsyncVectorEnv\n",
    "\n",
    "tax.set_platform('cpu')\n",
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "NENVS = 8\n",
    "\n",
    "def evaluation(rng, env, policy, niters: int = 5):\n",
    "    action_type = env.action_space.__class__.__name__\n",
    "    all_scores = []\n",
    "    for _ in range(niters):\n",
    "        observation, score = env.reset(), 0\n",
    "        for _ in range(env.spec.max_episode_steps):\n",
    "            rng, rng_action = jax.random.split(rng)\n",
    "            action = policy(rng, observation)\n",
    "            if action_type == 'Discrete':\n",
    "                action = int(action)\n",
    "            else:\n",
    "                action = np.asarray(action)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        all_scores.append(score)\n",
    "    info = {}\n",
    "    info['eval/score'] = np.mean(all_scores)\n",
    "    info['eval/score_std'] = np.std(all_scores)\n",
    "    return info\n",
    "\n",
    "\n",
    "def interaction(env, horizon: int = 10, seed: int = 42):\n",
    "    rng = jax.random.PRNGKey(seed)\n",
    "    observation, buf = env.reset(), []\n",
    "    policy = yield\n",
    "    \n",
    "    # -- Interaction Loop.\n",
    "    \n",
    "    while True:\n",
    "        for _ in range(horizon):\n",
    "            rng, rng_action = jax.random.split(rng)\n",
    "            action = np.array(policy(rng_action, observation))\n",
    "            observation_next, reward, done, info = env.step(action)\n",
    "            buf.append({\n",
    "                'observation': observation,\n",
    "                'reward': reward,\n",
    "                'terminal': 1. - done,\n",
    "                'action': action\n",
    "            })\n",
    "            observation = observation_next.copy()\n",
    "            \n",
    "        data = jit(tax.reduce)(buf)\n",
    "        data['last_observation'] = observation\n",
    "        policy = yield data\n",
    "        buf = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73866eff-8649-4f15-ba33-6772c333c638",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_env = lambda: gym.make('CartPole-v0')\n",
    "\n",
    "env              = AsyncVectorEnv([make_env for _ in range(NENVS)])\n",
    "env_test         = gym.make('CartPole-v0')\n",
    "action_size      = env_test.action_space.n\n",
    "observation_size = env_test.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3217410c-0a25-43e6-b1a3-1716f8b49048",
   "metadata": {},
   "source": [
    "# `Neural Network and Optimizers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3497fde-e2b9-40f1-8cdf-fe5b7bb1e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(42)\n",
    "dummy_action = jnp.zeros((action_size))\n",
    "dummy_observation = jnp.zeros((observation_size))\n",
    "\n",
    "policy_def = lambda x: tax.mlp_categorical(action_size)(x)\n",
    "policy_def = hk.transform(policy_def)\n",
    "policy_def = hk.without_apply_rng(policy_def)\n",
    "policy_opt = getattr(optax, 'adabelief')(learning_rate=1e-3)\n",
    "value_def  = lambda x: tax.mlp_deterministic(1)(x).squeeze(-1)\n",
    "value_def  = hk.transform(value_def)\n",
    "value_def  = hk.without_apply_rng(value_def)\n",
    "value_opt  = getattr(optax, 'adabelief')(learning_rate=1e-3)\n",
    "\n",
    "rng, rng_policy, rng_value = jax.random.split(rng, 3)\n",
    "value_params               = value_def.init(rng_policy, dummy_observation)\n",
    "value_opt_state            = value_opt.init(value_params)\n",
    "policy_params              = policy_def.init(rng_policy, dummy_observation)\n",
    "policy_opt_state           = policy_opt.init(policy_params)\n",
    "\n",
    "params    = {'policy': policy_params, 'value': value_params}\n",
    "opt_state = {'policy': policy_opt_state, 'value': value_opt_state}\n",
    "state     = State(params=params, opt_state=opt_state, key=rng)\n",
    "\n",
    "process_data2batch = partial(process_data, \n",
    "                             value_apply=value_def.apply,\n",
    "                             policy_apply=policy_def.apply)\n",
    "\n",
    "def _make_policy(p):\n",
    "    fn = lambda rng, x: policy_def.apply(p, x).sample(seed=rng) \n",
    "    return jit(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cb049e1-9776-4c83-8abc-7c935b575c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = partial(loss_ppo_def, \n",
    "               epsilon_ppo=0.2,\n",
    "               value_apply=value_def.apply, \n",
    "               policy_apply=policy_def.apply)\n",
    "\n",
    "\n",
    "@jit\n",
    "def update_fn(state, inputs):\n",
    "    \"\"\" Generic Update function \"\"\"\n",
    "    g, metrics = jax.grad(loss, has_aux=True)(state.params, inputs)\n",
    "\n",
    "    updates, value_opt_state = value_opt.update(g['value'], state.opt_state['value'])\n",
    "    value_params = jax.tree_multimap(lambda p, u: p + u, state.params['value'], updates)\n",
    "\n",
    "    updates, policy_opt_state = policy_opt.update(g['policy'], state.opt_state['policy'])\n",
    "    policy_params = jax.tree_multimap(lambda p, u: p + u, state.params['policy'], updates)\n",
    "\n",
    "    params = state.params\n",
    "    params = dict(policy=policy_params, value=value_params)\n",
    "    opt_state = state.opt_state\n",
    "    opt_state = dict(policy=policy_opt_state, value=value_opt_state)\n",
    "    state = state.replace(params=params, opt_state=opt_state)\n",
    "    return state, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2058c6fe-673f-4f40-a152-91c65ac551c6",
   "metadata": {},
   "source": [
    "# `Initialization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbc08def-6038-4f85-ba6d-87e7b0a62762",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_step = interaction(env, horizon=100)\n",
    "interaction_step.send(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bbf87c-b0fd-493e-aa40-2047ad00f632",
   "metadata": {},
   "source": [
    "# `Training Loop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6148ea-059a-4f6e-99e5-69a5c5480b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH     = 5\n",
    "MINIBATCH = 32\n",
    "\n",
    "@jit\n",
    "def _step(carry, xs):\n",
    "    state, batch = carry\n",
    "    minibatch = tree.map_structure(lambda v: v[xs], batch)\n",
    "    state, info = update_fn(state, minibatch)\n",
    "    carry = (state, batch)\n",
    "    return carry, info\n",
    "\n",
    "\n",
    "@jit\n",
    "def _fit(carry, xs):\n",
    "    state, batch, key = carry\n",
    "    n                 = batch['observation'].shape[0]\n",
    "    key, subkey       = jax.random.split(rng)\n",
    "    index             = jnp.arange(n)\n",
    "    indexes           = jax.random.permutation(subkey, index)\n",
    "    indexes           = jnp.stack(\n",
    "        jnp.array_split(indexes, n / MINIBATCH)\n",
    "    )\n",
    "\n",
    "    _carry = (state, batch)\n",
    "    _xs    = indexes\n",
    "    (state, batch), info = jax.lax.scan(_step, _carry, _xs)    \n",
    "    carry = (state, batch, key)\n",
    "    return carry, info\n",
    "\n",
    "import tqdm\n",
    "\n",
    "for _ in tqdm.notebook.trange(100):\n",
    "    for _ in range(10):\n",
    "        data  = interaction_step.send(_make_policy(state.params['policy']))\n",
    "        batch = jit(process_data2batch)(state.params, data)\n",
    "\n",
    "        # Update state.\n",
    "        rng, subrng               = jax.random.split(rng)\n",
    "        carry                     = (state, batch, subrng)\n",
    "        xs                        = jnp.arange(EPOCH)\n",
    "        (state, batch, key), info = _fit(carry, xs)\n",
    "        info = tree.map_structure(lambda v: jnp.mean(v), info)\n",
    "\n",
    "    policy_fn = _make_policy(state.params['policy'])\n",
    "    info_eval =  evaluation(rng, env_test, policy_fn)\n",
    "    print(info_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edcaf0c-5f30-438d-822f-70875ded7ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346258a6-217f-4ae6-b294-efcb3c243dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f677b892-6184-411e-bb4f-7bf428fc7367",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(params, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db54b71-177b-4acb-a009-b83757663a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebe1abc-eb56-478d-8769-5534e7439736",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "n           = batch['observation'].shape[0]\n",
    "key, subkey = jax.random.split(rng)\n",
    "index       = jnp.arange(n)\n",
    "indexes     = jax.random.permutation(subkey, index)\n",
    "indexes     = jnp.stack(\n",
    "    jnp.array_split(indexes, n / MINIBATCH)\n",
    ")\n",
    "\n",
    "carry = (state, batch)\n",
    "xs    = indexes\n",
    "(state, batch), info = jax.lax.scan(_step, carry, xs)\n",
    "info = tree.map_structure(lambda v: jnp.mean(v), info)\n",
    "\"\"\"\n",
    "\n",
    "rng, subrng               = jax.random.split(rng)\n",
    "carry                     = (state, batch, subrng)\n",
    "xs                        = jnp.arange(5)\n",
    "(state, batch, key), info = _fit(carry, xs)\n",
    "info = tree.map_structure(lambda v: jnp.mean(v), info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1961a8-6263-4bc4-8282-9fb8eaf8baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_fn = _make_policy(state.params['policy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1e7cf0-fd65-4982-a563-1c3cd857ee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_fn(rng, env_test.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d10638e-fcf6-412a-9db8-b6a0e29ef074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2441c61f-0cb4-405a-a70b-cdde1328d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(rng, env_test, policy_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da74eca-1402-4f1a-b15f-f220b124fb64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
