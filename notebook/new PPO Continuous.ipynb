{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b6980e4-1378-4267-be7b-eb17c9565a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import jax\n",
    "import tax\n",
    "import tree\n",
    "import optax\n",
    "import numpy as np\n",
    "import haiku as hk\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from ppo.ppo import Data\n",
    "from ppo.ppo import State\n",
    "from ppo.ppo import Batch\n",
    "from ppo.ppo import process_data\n",
    "from ppo.ppo import loss_ppo_def\n",
    "from ppo.common.utils import evaluation\n",
    "from jax import jit\n",
    "from jax import vmap\n",
    "from functools import partial\n",
    "from gym.vector import AsyncVectorEnv\n",
    "\n",
    "tax.set_platform('cpu')\n",
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "NENVS = 8\n",
    "\n",
    "def evaluation(rng, env, policy, niters: int = 5):\n",
    "    action_type = env.action_space.__class__.__name__\n",
    "    all_scores = []\n",
    "    for _ in range(niters):\n",
    "        observation, score = env.reset(), 0\n",
    "        for _ in range(env.spec.max_episode_steps):\n",
    "            rng, rng_action = jax.random.split(rng)\n",
    "            action = policy(rng, observation)\n",
    "            if action_type == 'Discrete':\n",
    "                action = int(action)\n",
    "            else:\n",
    "                action = np.asarray(action)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        all_scores.append(score)\n",
    "    info = {}\n",
    "    info['eval/score'] = np.mean(all_scores)\n",
    "    info['eval/score_std'] = np.std(all_scores)\n",
    "    return info\n",
    "\n",
    "\n",
    "def interaction(env, horizon: int = 10, seed: int = 42):\n",
    "    rng = jax.random.PRNGKey(seed)\n",
    "    observation, buf = env.reset(), []\n",
    "    policy = yield\n",
    "    \n",
    "    # -- Interaction Loop.\n",
    "    \n",
    "    while True:\n",
    "        for _ in range(horizon):\n",
    "            rng, rng_action = jax.random.split(rng)\n",
    "            action = np.array(policy(rng_action, observation))\n",
    "            observation_next, reward, done, info = env.step(action)\n",
    "            buf.append({\n",
    "                'observation': observation,\n",
    "                'reward': reward,\n",
    "                'terminal': 1. - done,\n",
    "                'action': action\n",
    "            })\n",
    "            observation = observation_next.copy()\n",
    "            \n",
    "        data = jit(tax.reduce)(buf)\n",
    "        data['last_observation'] = observation\n",
    "        policy = yield data\n",
    "        buf = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3848117d-bb13-4d64-a957-6038d7f31f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Pendulum-v0'\n",
    "make_env = lambda: gym.make(name)\n",
    "\n",
    "env              = AsyncVectorEnv([make_env for _ in range(NENVS)])\n",
    "env_test         = gym.make(name)\n",
    "action_size      = env_test.action_space.shape[0]\n",
    "observation_size = env_test.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0c0ed1-f2ef-4e96-af4b-ecc2128f49dc",
   "metadata": {},
   "source": [
    "# `Neural Network and Optimizers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd17fbfb-c4cf-4941-9da3-0860b8a5f522",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(42)\n",
    "dummy_action = jnp.zeros((action_size,))\n",
    "dummy_observation = jnp.zeros((observation_size,))\n",
    "\n",
    "policy_def = lambda x: tax.mlp_multivariate_normal_diag(\n",
    "    action_size, logstd_min=-10.0, logstd_max=3.0)(x)\n",
    "policy_def = hk.transform(policy_def)\n",
    "policy_def = hk.without_apply_rng(policy_def)\n",
    "policy_opt = getattr(optax, 'adabelief')(learning_rate=5e-4)\n",
    "value_def  = lambda x: tax.mlp_deterministic(1)(x).squeeze(-1)\n",
    "value_def  = hk.transform(value_def)\n",
    "value_def  = hk.without_apply_rng(value_def)\n",
    "value_opt  = getattr(optax, 'adabelief')(learning_rate=5e-4)\n",
    "\n",
    "rng, rng_policy, rng_value = jax.random.split(rng, 3)\n",
    "value_params               = value_def.init(rng_policy, dummy_observation)\n",
    "value_opt_state            = value_opt.init(value_params)\n",
    "policy_params              = policy_def.init(rng_policy, dummy_observation)\n",
    "policy_opt_state           = policy_opt.init(policy_params)\n",
    "\n",
    "params    = {'policy': policy_params, 'value': value_params}\n",
    "opt_state = {'policy': policy_opt_state, 'value': value_opt_state}\n",
    "state     = State(params=params, opt_state=opt_state, key=rng)\n",
    "\n",
    "process_data2batch = partial(process_data, \n",
    "                             value_apply=value_def.apply,\n",
    "                             policy_apply=policy_def.apply)\n",
    "\n",
    "def _make_policy(p):\n",
    "    fn = lambda rng, x: policy_def.apply(p, x).sample(seed=rng) \n",
    "    return jit(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f78ead6d-f504-4d3c-9c9c-501065839a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = partial(loss_ppo_def, \n",
    "               epsilon_ppo=0.2,\n",
    "               entropy_cost=0.1,\n",
    "               value_cost=1,\n",
    "               value_apply=value_def.apply, \n",
    "               policy_apply=policy_def.apply)\n",
    "\n",
    "\n",
    "@jit\n",
    "def update_fn(state, inputs):\n",
    "    \"\"\" Generic Update function \"\"\"\n",
    "    g, metrics = jax.grad(loss, has_aux=True)(state.params, inputs)\n",
    "\n",
    "    updates, value_opt_state = value_opt.update(g['value'], state.opt_state['value'])\n",
    "    value_params = jax.tree_multimap(lambda p, u: p + u, state.params['value'], updates)\n",
    "\n",
    "    updates, policy_opt_state = policy_opt.update(g['policy'], state.opt_state['policy'])\n",
    "    policy_params = jax.tree_multimap(lambda p, u: p + u, state.params['policy'], updates)\n",
    "\n",
    "    params = state.params\n",
    "    params = dict(policy=policy_params, value=value_params)\n",
    "    opt_state = state.opt_state\n",
    "    opt_state = dict(policy=policy_opt_state, value=value_opt_state)\n",
    "    state = state.replace(params=params, opt_state=opt_state)\n",
    "    return state, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e54bfa0-668b-4bac-a171-a16ae4915b95",
   "metadata": {},
   "source": [
    "# `Initialization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7838c53-c02e-40d3-879f-39e0f13a00c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_step = interaction(env, horizon=100)\n",
    "interaction_step.send(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affd4fe0-be36-4af6-a9e8-fa7008df4025",
   "metadata": {},
   "source": [
    "# `Training Loop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5130a1-3949-480f-bbc6-6d4d19d73109",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH     = 10\n",
    "MINIBATCH = 32\n",
    "\n",
    "@jit\n",
    "def _step(carry, xs):\n",
    "    state, batch = carry\n",
    "    minibatch = tree.map_structure(lambda v: v[xs], batch)\n",
    "    state, info = update_fn(state, minibatch)\n",
    "    carry = (state, batch)\n",
    "    return carry, info\n",
    "\n",
    "\n",
    "@jit\n",
    "def _fit(carry, xs):\n",
    "    state, batch, key = carry\n",
    "    n                 = batch['observation'].shape[0]\n",
    "    key, subkey       = jax.random.split(rng)\n",
    "    index             = jnp.arange(n)\n",
    "    indexes           = jax.random.permutation(subkey, index)\n",
    "    indexes           = jnp.stack(\n",
    "        jnp.array_split(indexes, n / MINIBATCH)\n",
    "    )\n",
    "\n",
    "    _carry = (state, batch)\n",
    "    _xs    = indexes\n",
    "    (state, batch), info = jax.lax.scan(_step, _carry, _xs)    \n",
    "    carry = (state, batch, key)\n",
    "    return carry, info\n",
    "\n",
    "import tqdm\n",
    "\n",
    "for _ in tqdm.notebook.trange(100):\n",
    "    for _ in range(10):\n",
    "        data  = interaction_step.send(_make_policy(state.params['policy']))\n",
    "        batch = jit(process_data2batch)(state.params, data)\n",
    "\n",
    "        # Update state.\n",
    "        rng, subrng               = jax.random.split(rng)\n",
    "        carry                     = (state, batch, subrng)\n",
    "        xs                        = jnp.arange(EPOCH)\n",
    "        (state, batch, key), info = _fit(carry, xs)\n",
    "        info = tree.map_structure(lambda v: jnp.mean(v), info)\n",
    "\n",
    "    policy_fn = _make_policy(state.params['policy'])\n",
    "    info_eval =  evaluation(rng, env_test, policy_fn)\n",
    "    print({**info_eval, **info})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff761d-5c31-40c1-aa35-35494824d18c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
